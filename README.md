# ASL-Deep-Learning

This Project is about detection of American Sign Language, initially detecting letters and displaying it, and then determining words with Real-time detection implementation.

Updates:
# Introduction
According to the Communication Service for the Deaf (CSD) [1], there are 360 million deaf people worldwide. Another report by the World Health Organization (WHO) [2] bumps up the number to 466 million people or over 6% of the world's population suffering from disabling hearing loss. Approximately 70 million people around the world are deaf-mute. While translation services have
become easily accessible for about 100 languages, sign language is still an area that has not been explored. Our goal is to detect & translate the letters of ASL in real-time.

# Technologies used

# About Architecture
To get foundation for the model configuration used a popular approach in deep learning called Transfer learning approach.
![image](https://user-images.githubusercontent.com/90703475/224968708-e2a7dd1b-879a-4033-beaf-55c6d05677ae.png)

Mainly Focusing on MobileNetV2 Architecture.



# About Dataset

# Performance of Model

# Conclusion
# References
[1] C. Soukup, \Communication service for the deaf,"
1975. https://www.csd.org/, Last accessed on
2021-05-31.

[2] R. E. C. David W Dowdy, \World health
organization. office of library and health literature
services," 1988. https://www.who.int/, Last
accessed on 2021-05-31.

# Project Mentors
# Project Mentees
